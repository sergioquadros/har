{
    "collab_server" : "",
    "contents" : "---\ntitle: \"HAR: A Machine Learning Essay on Human Activity Recognition.\"\nauthor: \"Sergio H. S. de Quadros\"\ndate: \"2016-03-21\"\noutput: \n  html_document: \n    fig_caption: yes\n---\n\nRemarks on reproducibility: R version 3.2.4 at _x86-64-pc-linux-gnu (64-bit)_ platform running under _Ubuntu 14.04.4 LTS_ and using the folowing libraries:  \n\n```{r setup, message=FALSE, echo=TRUE}\nlibrary(knitr);library(rmarkdown);library(ggplot2);library(magrittr);library(caret);library(gridExtra); library(RCurl);library(corrplot);library(e1071);library(rpart);library(randomForest)\n```\n\nI achievied data set from  <http://groupware.les.inf.puc-rio.br/har> on _the Weight Lifting Exercise Dataset_ with:  \n\n```{r original_data , message=FALSE, echo=TRUE}\n# file names and url\nURLtrain <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\nURLtest <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\nNAMEtrain <- \"pml_training.csv\"\nNAMEtest <- \"pml_testing.csv\"\n# Create directory\nif (!file.exists(\"./figures\")) {\n        dir.create(\"./figures\")\n}\n# Download\nif (!file.exists(NAMEtrain)) {\n        download.file(URLtrain, destfile=NAMEtrain)\n}\nif (!file.exists(NAMEtest)) {\n        download.file(URLtest, destfile=NAMEtest)\n}\n# Load data and clean 'NA' & meaningless features\ntraining <- read.csv(NAMEtrain, na.strings = c(\"NA\", \"\"))\ntesting <- read.csv(NAMEtest, na.strings = c(\"NA\", \"\"))\ntraining1 <- training[, colSums(is.na(training)) == 0]\ntesting1 <- testing[, colSums(is.na(testing)) == 0]\ntraining0 <- training1[, -c(1:7)]\ntesting0 <- testing1[, -c(1:7)]\n```\n\nThe training data set has `r dim(training)[1]` examples with `r dim(training)[2]-1` features in a supervised multiclass problem, the outcome has seven outcome classes: `r unique(training0$classe)` corresponding to following activities:  sitting, standing, standing up, sitting down and walking. Our test set has  `r dim(testing)[1]` examples. \n\nI selected `r dim(training0)[2]-1` features by exclusion of _NA_ and meaningless ones, then I looked for high covariances between features (PCA?), skeewness(Box Cox transformations?) and distribution in the classes of outcome, but I didn't made those transformations because they wouldn't add accuracy for a non-linear multiclass models.\n\n```{r exloratory, message=FALSE, echo=FALSE}\n# There were some clumps with high covariances between features\nM <- cor(training0[, c(1:52)])\nord <- corrMatOrder(M, order=\"AOE\")\nM2 <- M[ord,ord]\ncorrplot(M2)\n# # Skeewness\n# # sapply(training0[, c(1:52)],skeewness)\n# # The classe outcome has this distribution:\nhistogram(training$classe)\nfeaturePlot(training0[, c(1:52)], training0[, c(53)], \"strip\")\n```\n\nSo I divided the training set into two subsets: \n\n+    65% for prediction and cross-validation;   \n\n+    35% to compute the out-of-sample errors.  \n\n```{r split_control, message=FALSE, echo=TRUE}\nfitControl <- trainControl(method = \"cv\", number=7)\nset.seed(141593)\ninTrain <- createDataPartition(training0$classe, p = 0.65)[[1]]\ntrainSub <- training0[ inTrain,]\ntestSub <- training0[-inTrain,]\n```\n\nI can approach a multiclass classification problem with logistic regression, SVM, random forest, decision trees, k-nearest neighbors and so on. My first choice was the fast _k-nearest neighbors_. Below we have out-of-sample optimistic assessment for k-nearest neighbors models' accuracies:\n\n```{r knn_model, message=FALSE, echo=TRUE}\nset.seed(141593)\nmod_knn <- train(classe ~.,method=\"knn\",trControl=fitControl,data=trainSub)\npred_knn <- predict(mod_knn,testSub)\npredDF <- data.frame(pred_knn,testSub$classe)\nuu <- confusionMatrix(pred_knn, testSub$classe)\nuu\n```\n\nI tried another methods: _rpart_ , _glm_ and _gbm_ that the accuracies was about 50%. At last I used the _random forest method_ - more accurate, but slower. Below we have out-of-sample optimistic assessment for random forest models' accuracies:\n\n```{r randomForest_model, message=FALSE, echo=TRUE}\nset.seed(141593)\nmod_rforest <- train(classe ~.,method=\"rf\", trControl=fitControl,data=trainSub)\npred_rforest <- predict(mod_rforest,testSub)\npredDF2 <- data.frame(pred_rforest,testSub$classe)\nvv <- confusionMatrix(pred_rforest, testSub$classe)\nvv\n```\n\nThis table summarizes _in_ and _out-of-sample_ errors by :  \n\n| Methods  | _In_ Accuracy (%) | _Out-sample_ Accuracy (%) |\n| ------------- | ------------- | ------------- |\n| k-nearest neighbors | `r round(100*mod_knn$resample$Accuracy[1],2)`  | `r round(100*uu$overall[1],2)` |\n| Random Forest | `r round(100*mod_rforest$resample$Accuracy[1],3)`  | `r round(100*vv$overall[1],3)` |\n\nThis figure presents in another way the results:\n\n```{r results_crossvalidation}\ntitulo <- \"Cross-validation Accuracy\"\ntrellis.par.set(caretTheme())\np3 <- plot(mod_knn)\ntrellis.par.set(caretTheme())\np4 <- plot(mod_rforest)\np5 <- ggplot(predDF,aes(x=pred_knn,y=testSub$classe))+geom_jitter(aes(colour=testSub$classe))+xlab(\"Class Predict\")+ylab(\"Actual\")+ggtitle(\"k-Nearest Neighbors\")+ theme(legend.title=element_blank())\np6 <- ggplot(predDF2,aes(x=pred_rforest,y=testSub$classe))+\n        geom_jitter(aes(colour=testSub$classe))+xlab(\"Class Predict\")+ylab(\"Actual\")+ggtitle(\"Random Forest\")+theme(legend.title=element_blank())\ngrid.arrange(p3,p4,p5,p6, ncol = 2, nrow = 2)\n```\n**Figure 1** _Top_ Both with accuracies by cross-validated k=7 preprocess: _left_ k-Nearest Neighbors models; _rigth_ random forest. _Bottom_ Out-of-sample accuracies: _left_ k-Nearest Neighbors models; _rigth_ random forest. Random Forest model have better performance than k-Nearest Neighbors ones.  \n\nWe must choose the _Random Forest model_ for new predictions with testing set:\n`r predict(mod_rforest, testing0)`\n\n# Bibliography  \n\n+    Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises.](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201) Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\n\n+    Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. [Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements.](http://groupware.les.inf.puc-rio.br/work.jsf?p1=10335) Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. \n\n+    Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises.](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201) Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\n\n+    Ugulino, W.; Ferreira, M.; Velloso, E.; Fuks, H. [Virtual Caregiver: Colaboração de Parentes no Acompanhamento de Idosos.](http://groupware.les.inf.puc-rio.br/work.jsf?p1=10657) Anais do SBSC 2012, IX Simpósio Brasileiro de Sistemas Colaborativos , pp. 43-48. São Paulo, SP: IEEE, 2012. ISBN 978-0-7695-4890-6.\n",
    "created" : 1458595161086.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2696683798",
    "id" : "CEB35330",
    "lastKnownWriteTime" : 1459079536,
    "last_content_update" : 1459079536649,
    "path" : "~/Documentos/dataScience/datascienceJH/ml_JH032016/work/har/har.Rmd",
    "project_path" : "har.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}