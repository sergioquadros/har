{
    "collab_server" : "",
    "contents" : "####\n# regression subset selection in the prostate dataset\nlibrary(caret)\nlibrary(ElemStatLearn)\nlibrary(png)\ndata(prostate)\n\ncovnames <- names(prostate[-(9:10)])\ny <- prostate$lpsa\nx <- prostate[,covnames]\n\nform <- as.formula(paste(\"lpsa~\", paste(covnames, collapse=\"+\"), sep=\"\"))\nsummary(lm(form, data=prostate[prostate$train,]))\n\nset.seed(1)\ntrain.ind <- sample(nrow(prostate), ceiling(nrow(prostate))/2)\ny.test <- prostate$lpsa[-train.ind]\nx.test <- x[-train.ind,]\n\ny <- prostate$lpsa[train.ind]\nx <- x[train.ind,]\n\np <- length(covnames)\nrss <- list()\nfor (i in 1:p) {\n        cat(i)\n        Index <- combn(p,i)\n        \n        rss[[i]] <- apply(Index, 2, function(is) {\n                form <- as.formula(paste(\"y~\", paste(covnames[is], collapse=\"+\"), sep=\"\"))\n                isfit <- lm(form, data=x)\n                yhat <- predict(isfit)\n                train.rss <- sum((y - yhat)^2)\n                \n                yhat <- predict(isfit, newdata=x.test)\n                test.rss <- sum((y.test - yhat)^2)\n                c(train.rss, test.rss)\n        })\n}\n# criei um arquivo 'Plots'\npng(\"Plots/selection-plots-01.png\", height=432, width=432, pointsize=12)\nplot(1:p, 1:p, type=\"n\", ylim=range(unlist(rss)), xlim=c(0,p), xlab=\"number of predictors\", ylab=\"residual sum of squares\", main=\"Prostate cancer data\")\nfor (i in 1:p) {\n        points(rep(i-0.15, ncol(rss[[i]])), rss[[i]][1, ], col=\"blue\")\n        points(rep(i+0.15, ncol(rss[[i]])), rss[[i]][2, ], col=\"red\")\n}\nminrss <- sapply(rss, function(x) min(x[1,]))\nlines((1:p)-0.15, minrss, col=\"blue\", lwd=1.7)\nminrss <- sapply(rss, function(x) min(x[2,]))\nlines((1:p)+0.15, minrss, col=\"red\", lwd=1.7)\nlegend(\"topright\", c(\"Train\", \"Test\"), col=c(\"blue\", \"red\"), pch=1)\ndev.off()\n\n##\n# ridge regression on prostate dataset\nlibrary(MASS)\nlambdas <- seq(0,50,len=10)\nM <- length(lambdas)\ntrain.rss <- rep(0,M)\ntest.rss <- rep(0,M)\nbetas <- matrix(0,ncol(x),M)\nfor(i in 1:M){\n        Formula <-as.formula(paste(\"y~\",paste(covnames,collapse=\"+\"),sep=\"\"))\n        fit1 <- lm.ridge(Formula,data=x,lambda=lambdas[i])\n        betas[,i] <- fit1$coef\n        \n        scaledX <- sweep(as.matrix(x),2,fit1$xm)\n        scaledX <- sweep(scaledX,2,fit1$scale,\"/\")\n        yhat <- scaledX%*%fit1$coef+fit1$ym\n        train.rss[i] <- sum((y - yhat)^2)\n        \n        scaledX <- sweep(as.matrix(x.test),2,fit1$xm)\n        scaledX <- sweep(scaledX,2,fit1$scale,\"/\")\n        yhat <- scaledX%*%fit1$coef+fit1$ym\n        test.rss[i] <- sum((y.test - yhat)^2)\n}\n\npng(file=\"Plots/selection-plots-02.png\", width=432, height=432, pointsize=12) \nplot(lambdas,test.rss,type=\"l\",col=\"red\",lwd=2,ylab=\"RSS\",ylim=range(train.rss,test.rss))\nlines(lambdas,train.rss,col=\"blue\",lwd=2,lty=2)\nbest.lambda <- lambdas[which.min(test.rss)]\nabline(v=best.lambda+1/9)\nlegend(30,30,c(\"Train\",\"Test\"),col=c(\"blue\",\"red\"),lty=c(2,1))\ndev.off()\n\n\npng(file=\"Plots/selection-plots-03.png\", width=432, height=432, pointsize=8) \nplot(lambdas,betas[1,],ylim=range(betas),type=\"n\",ylab=\"Coefficients\")\nfor(i in 1:ncol(x))\n        lines(lambdas,betas[i,],type=\"b\",lty=i,pch=as.character(i))\nabline(h=0)\nlegend(\"topright\",covnames,pch=as.character(1:8))\ndev.off()\n\n\n#######\n# lasso\nlibrary(lars)\nlasso.fit <- lars(as.matrix(x), y, type=\"lasso\", trace=TRUE)\n\npng(file=\"Plots/selection-plots-04.png\", width=432, height=432, pointsize=8) \nplot(lasso.fit, breaks=FALSE)\nlegend(\"topleft\", covnames, pch=8, lty=1:length(covnames), col=1:length(covnames))\ndev.off()\n\n# this plots the cross validation curve\npng(file=\"Plots/selection-plots-05.png\", width=432, height=432, pointsize=12) \nlasso.cv <- cv.lars(as.matrix(x), y, K=10, type=\"lasso\", trace=TRUE)\ndev.off()\n\nM <- cor(mtcars)\nord <- corrMatOrder(M, order=\"AOE\")\nM2 <- M[ord,ord]\npar(ask=TRUE)\ncorrplot.mixed(M2)\ncorrplot.mixed(M2, lower=\"ellipse\", upper=\"circle\")\ncorrplot.mixed(M2, lower=\"square\", upper=\"circle\")\ncorrplot.mixed(M2, lower=\"shade\", upper=\"circle\")\ncorrplot.mixed(M2, tl.pos=\"lt\")\ncorrplot.mixed(M2, tl.pos=\"lt\", diag=\"u\")\ncorrplot.mixed(M2, tl.pos=\"lt\", diag=\"l\")\ncorrplot.mixed(M2, tl.pos=\"n\")\ncorrplot(M2)\n\n# COMBINING\n# Example with Wage data\n# Create training, test and validation sets\nlibrary(ISLR); data(Wage); library(ggplot2); library(caret);\nWage <- subset(Wage,select=-c(logwage))\n# Create a building data set and validation set\ninBuild <- createDataPartition(y=Wage$wage,\n                               p=0.7, list=FALSE)\nvalidation <- Wage[-inBuild,]; buildData <- Wage[inBuild,]\n\ninTrain <- createDataPartition(y=buildData$wage,\n                               p=0.7, list=FALSE)\ntraining <- buildData[inTrain,]; testing <- buildData[-inTrain,]\n# dim(Wage)\n# [1] 3000   11\n# dim(buildData)\n# [1] 2102   11\n# dim(validation)\n# [1] 898  11\n# dim(testing)\n# [1] 628  11\n# dim(training)\n# [1] 1474   11\n\n# Build two different models\nmod1 <- train(wage ~.,method=\"glm\",data=training)\nmod2 <- train(wage ~.,method=\"rf\",\n              data=training, \n              trControl = trainControl(method=\"cv\"),number=3)\n# Predict on the testing set\npred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)\nqplot(pred1,pred2,colour=wage,data=testing)\n# Fit a model that combines predictors\npredDF <- data.frame(pred1,pred2,wage=testing$wage)\ncombModFit <- train(wage ~.,method=\"gam\",data=predDF)\ncombPred <- predict(combModFit,predDF)\n# Testing errors\nsqrt(sum((pred1-testing$wage)^2))\nsqrt(sum((pred2-testing$wage)^2))\nsqrt(sum((combPred-testing$wage)^2))\n# mod1\n# RMSE      Rsquared   RMSE SD   Rsquared SD\n# 34.78051  0.3304368  1.847303  0.02788085 \n\n# mod2\n# mtry  RMSE      Rsquared   RMSE SD   Rsquared SD\n# 13    35.16514  0.3237126  4.741134  0.07529428 \n# RMSE was used to select the optimal model using  the smallest value.\n# The final value used for the model was mtry = 13.\n\n# combModFit\n# Generalized Additive Model using Splines \n# select  RMSE      Rsquared   RMSE SD   Rsquared SD\n# FALSE   33.04432  0.3625878  2.701966  0.06760900 \n# TRUE    33.04914  0.3626146  2.710643  0.06736376 \n# Tuning parameter 'method' was held constant at a value of GCV.Cp\n# RMSE was used to select the optimal model using  the smallest value.\n# The final values used for the model were select = FALSE and method = GCV.Cp\n\n# Predict on validation data set\npred1V <- predict(mod1,validation); pred2V <- predict(mod2,validation)\npredVDF <- data.frame(pred1=pred1V,pred2=pred2V)\ncombPredV <- predict(combModFit,predVDF)\n# Evaluate on validation\nsqrt(sum((pred1V-validation$wage)^2))\n# [1] 1040.383\nsqrt(sum((pred2V-validation$wage)^2))\n# [1] 1078.505\nsqrt(sum((combPredV-validation$wage)^2))\n# [1] 1038.768\n\n# Time series data: Forecasting\n# Google data\nlibrary(quantmod)\nlibrary(quad)\nlibrary(Quandl)\nfrom.dat <- as.Date(\"01/01/08\", format=\"%m/%d/%y\")\nto.dat <- as.Date(\"12/31/13\", format=\"%m/%d/%y\")\ngetSymbols(\"GOOG\", src=\"google\", from = from.dat, to = to.dat)\nhead(GOOG)\n# Summarize monthly and store as time series\nmGoog <- to.monthly(GOOG)# Error in `index<-.xts\ngoogOpen <- Op(mGoog)\nts1 <- ts(googOpen,frequency=12)\nplot(ts1,xlab=\"Years+1\", ylab=\"GOOG\")\n# Decompose a time series into parts\nplot(decompose(ts1),xlab=\"Years+1\")\n# Training and test sets\nts1Train <- window(ts1,start=1,end=5)\nts1Test <- window(ts1,start=5,end=(7-0.01))\nts1Train\n# Simple moving average\nplot(ts1Train)\nlines(ma(ts1Train,order=3),col=\"red\")\n# Exponential smoothing\nets1 <- ets(ts1Train,model=\"MMM\")\nfcast <- forecast(ets1)\nplot(fcast); lines(ts1Test,col=\"red\")\n# Get the accuracy\naccuracy(fcast,ts1Test)\n\n# UNSUPERVISED PREDICTION\n# Iris example ignoring species labels\ndata(iris); library(ggplot2)\ninTrain <- createDataPartition(y=iris$Species,\n                               p=0.7, list=FALSE)\ntraining <- iris[inTrain,]\ntesting <- iris[-inTrain,]\ndim(training); dim(testing)\n# Cluster with k-means\nkMeans1 <- kmeans(subset(training,select=-c(Species)),iter.max = 1000000,centers=3)\ntraining$clusters <- as.factor(kMeans1$cluster)\nqplot(Petal.Width,Petal.Length,colour=clusters,data=training)\n# Compare to real labels\ntable(kMeans1$cluster,training$Species)\n# Build predictor\nmodFit <- train(clusters ~.,data=subset(training,select=-c(Species)),method=\"rpart\")\ntable(predict(modFit,training),training$Species)\n# Apply on test\ntestClusterPred <- predict(modFit,testing) \ntable(testClusterPred ,testing$Species)\n\n\n",
    "created" : 1459047686440.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3229136546",
    "id" : "4CCC0C7A",
    "lastKnownWriteTime" : 1458867220,
    "last_content_update" : 1458867220,
    "path" : "~/Documentos/dataScience/datascienceJH/ml_JH032016/week4/week4.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}